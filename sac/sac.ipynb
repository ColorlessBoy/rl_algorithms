{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit1bdb8eb7d9a9428ab4af403dba516126",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from model import ValueNetwork, QNetwork, GaussianPolicy, DeterministicPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replay Memory\n",
    "Replay memory stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "- Transition - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
    "- ReplayMemory - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'mask'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils: Soft_update and Hard_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAC\n",
    "\n",
    "The algorithm is a V-based method.\n",
    "\n",
    "- First we have three net: $V(s; \\theta_V)$, $Q(s, a; \\theta_Q)$ and $\\pi(a \\vert s; \\theta_\\pi)$.\n",
    "\n",
    "- We want $Q(s, a; \\theta_Q) = Q_V = \\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V) \\right)$;\n",
    "\n",
    "  $$\n",
    "  J(\\theta_Q) = \\mathbb{E}_{(s, a) \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} (\\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V)\\right) - Q(s, a; \\theta_Q) )^2 \\right\\};\n",
    "  $$\n",
    "\n",
    "- We want $\\pi(a \\vert s; \\theta_\\pi) = \\pi^*_{Q, soft}(a \\vert s) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a'; \\theta_Q)}$;\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  J(\\theta_\\pi) =& \\mathbb{E}_{s \\sim \\mathcal{D}}\\left\\{D_{KL} \\left(\\pi(\\cdot \\vert s; \\theta_\\pi) \\Vert \\pi^*_{Q, soft}(s, \\cdot) \\right)\\right\\} \\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - log(\\pi^*_{Q, soft}(a \\vert s))\\right\\}\\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - \\frac{1}{\\alpha} Q(s, a;\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  If we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$\n",
    "\n",
    "- We want $V(s; \\theta_V) = T^\\pi_{soft} V(s; \\theta_V) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]$;\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta_V) =& \\mathbb{E}_{s \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} \\left(V(s;\\theta_V) - \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]\\right)^2 \\right\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The paper use two Value-Net: $V(s; \\theta_V)$ and $\\hat V(s; \\theta_{\\hat V})$, and $\\theta_{\\hat V} = (1 - \\tau) \\theta_{\\hat V} + \\tau \\theta_V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.value = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        self.value_optim = Adam(self.value.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.value_target = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        hard_update(self.value_target, self.value)\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_target_batch = reward_batch + self.gamma * self.value_target(next_state_batch)\n",
    "\n",
    "            \n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, q_target_batch) + F.mse_loss(q2_batch, q_target_batch)\n",
    "\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            v_target = min_q_a - self.alpha * log_p_a\n",
    "        v = self.value(state_batch)\n",
    "        value_loss = F.mse_loss(v, v_target)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        self.value_optim.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.value_target, self.value, self.tau)\n",
    "        \n",
    "        return critic_loss + value_loss, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9f4ef0caa616>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9f4ef0caa616>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    - $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## SAC2\n",
    "\n",
    "- $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\n",
    "- $\\pi(a\\vert s; \\theta_\\pi) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a' ; \\theta_Q)}$.\n",
    "\n",
    "However we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC2(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action_batch, log_p_next_action_batch, _ = self.policy.sample(next_state_batch)\n",
    "            q1_next_target_batch, q2_next_target_batch = self.critic_target(next_state_batch, next_action_batch)\n",
    "            min_q_next_target_batch = torch.min(q1_next_target_batch, q2_next_target_batch) - self.alpha * log_p_next_action_batch\n",
    "            next_q_batch = reward_batch  + mask_batch * self.gamma * min_q_next_target_batch\n",
    "\n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, next_q_batch) + F.mse_loss(q2_batch, next_q_batch)\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        \n",
    "        return critic_loss, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Actor-Critic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_train(algorithm, args):\n",
    "    env = gym.make(args['env_name'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    env.seed(args['seed'])\n",
    "\n",
    "    agent = algorithm(env.observation_space.shape[0], env.action_space, args)\n",
    "    memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "    total_numsteps = 0\n",
    "    updates = 0\n",
    "\n",
    "    for i_epsisode in range(1, args['num_steps']+1):\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "        while not done:\n",
    "            if args['start_steps'] > total_numsteps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            \n",
    "            if len(memory) > args['batch_size']:\n",
    "                for i in range(args['updates_per_step']):\n",
    "                    critic_loss, actor_loss = agent.update_parameters(memory, args['batch_size'], updates)\n",
    "                    updates += 1\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_steps += 1\n",
    "            total_numsteps += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "            memory.push(state, action, reward, next_state, mask)\n",
    "            state = next_state\n",
    "        \n",
    "        yield total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'env_name'              : 'HalfCheetah-v2',\n",
    "    'policy_type'           : 'Gaussian',\n",
    "    'gamma'                 : 0.99,\n",
    "    'tau'                   : 0.005,\n",
    "    'lr'                    : 0.0003,\n",
    "    'alpha'                 : 0.2,\n",
    "    'seed'                  : 0,\n",
    "    'batch_size'            : 256,\n",
    "    'num_steps'             : 1000000,\n",
    "    'hidden_size'           : 256,\n",
    "    'updates_per_step'      : 1,\n",
    "    'start_steps'           : 10000,\n",
    "    'target_update_interval': 1,\n",
    "    'replay_size'           : 1000000,\n",
    "    'cuda'                  : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total_steps       1000: EpReward =     -293.193741, Critic_Loss =   0.264792, Actor_Loss =  -1.741519\nTotal_steps       2000: EpReward =     -216.255512, Critic_Loss =   0.728227, Actor_Loss =  -5.289030\nTotal_steps       3000: EpReward =     -195.122628, Critic_Loss =   1.011094, Actor_Loss =  -7.781087\nTotal_steps       4000: EpReward =     -238.438604, Critic_Loss =   1.275122, Actor_Loss = -10.069914\nTotal_steps       5000: EpReward =     -370.436533, Critic_Loss =   1.395600, Actor_Loss = -12.626404\nTotal_steps       6000: EpReward =     -415.698217, Critic_Loss =   2.207469, Actor_Loss = -14.447821\nTotal_steps       7000: EpReward =     -241.467983, Critic_Loss =   2.948436, Actor_Loss = -17.421043\nTotal_steps       8000: EpReward =     -259.022695, Critic_Loss =   2.312378, Actor_Loss = -19.919596\nTotal_steps       9000: EpReward =     -237.559380, Critic_Loss =   2.903669, Actor_Loss = -23.477406\nTotal_steps      10000: EpReward =     -273.577534, Critic_Loss =   2.873574, Actor_Loss = -26.588306\nTotal_steps      11000: EpReward =     -242.668667, Critic_Loss =   2.951073, Actor_Loss = -28.740881\nTotal_steps      12000: EpReward =     -222.418801, Critic_Loss =   2.868883, Actor_Loss = -30.190250\nTotal_steps      13000: EpReward =     -261.552570, Critic_Loss =   3.095079, Actor_Loss = -29.647980\nTotal_steps      14000: EpReward =     -231.019891, Critic_Loss =   2.868243, Actor_Loss = -30.235437\nTotal_steps      15000: EpReward =     -259.448651, Critic_Loss =   3.299996, Actor_Loss = -30.361336\nTotal_steps      16000: EpReward =     -290.444809, Critic_Loss =   2.712530, Actor_Loss = -29.538187\nTotal_steps      17000: EpReward =      -91.814082, Critic_Loss =   2.026616, Actor_Loss = -29.562338\nTotal_steps      18000: EpReward =      -38.816924, Critic_Loss =   2.554685, Actor_Loss = -30.121521\nTotal_steps      19000: EpReward =       -2.328477, Critic_Loss =   2.136168, Actor_Loss = -30.757488\nTotal_steps      20000: EpReward =       56.138265, Critic_Loss =   2.732038, Actor_Loss = -30.444000\nTotal_steps      21000: EpReward =      -18.832283, Critic_Loss =   1.996010, Actor_Loss = -30.344578\nTotal_steps      22000: EpReward =       15.025617, Critic_Loss =   2.282972, Actor_Loss = -29.723137\nTotal_steps      23000: EpReward =       61.284640, Critic_Loss =   2.080605, Actor_Loss = -30.201206\nTotal_steps      24000: EpReward =      112.390222, Critic_Loss =   1.948302, Actor_Loss = -30.837624\nTotal_steps      25000: EpReward =       56.626734, Critic_Loss =   2.293711, Actor_Loss = -31.198021\nTotal_steps      26000: EpReward =      134.355353, Critic_Loss =   2.014277, Actor_Loss = -31.304184\nTotal_steps      27000: EpReward =      -64.192935, Critic_Loss =   2.001834, Actor_Loss = -31.543198\nTotal_steps      28000: EpReward =       27.728887, Critic_Loss =   2.107719, Actor_Loss = -31.513294\nTotal_steps      29000: EpReward =      -38.510592, Critic_Loss =   2.118955, Actor_Loss = -32.485180\nTotal_steps      30000: EpReward =     -139.427204, Critic_Loss =   1.827669, Actor_Loss = -31.419506\nTotal_steps      31000: EpReward =       25.352479, Critic_Loss =   1.697777, Actor_Loss = -32.002197\nTotal_steps      32000: EpReward =     -125.274400, Critic_Loss =   1.979548, Actor_Loss = -32.961105\nTotal_steps      33000: EpReward =        2.721856, Critic_Loss =   1.936428, Actor_Loss = -32.701508\nTotal_steps      34000: EpReward =      120.260298, Critic_Loss =   2.122449, Actor_Loss = -33.335003\nTotal_steps      35000: EpReward =       13.152268, Critic_Loss =   1.778881, Actor_Loss = -33.656769\nTotal_steps      36000: EpReward =      -44.452034, Critic_Loss =   1.834123, Actor_Loss = -33.375092\nTotal_steps      37000: EpReward =       51.648587, Critic_Loss =   1.864276, Actor_Loss = -34.135849\nTotal_steps      38000: EpReward =      -70.544007, Critic_Loss =   2.008397, Actor_Loss = -34.225616\nTotal_steps      39000: EpReward =      -80.366903, Critic_Loss =   1.665686, Actor_Loss = -34.473717\nTotal_steps      40000: EpReward =       55.278862, Critic_Loss =   2.045607, Actor_Loss = -35.058128\nTotal_steps      41000: EpReward =      -11.768956, Critic_Loss =   2.332248, Actor_Loss = -34.833511\nTotal_steps      42000: EpReward =      293.167923, Critic_Loss =   2.181968, Actor_Loss = -36.608086\nTotal_steps      43000: EpReward =      305.769347, Critic_Loss =   1.983217, Actor_Loss = -35.379791\nTotal_steps      44000: EpReward =       43.633427, Critic_Loss =   1.820558, Actor_Loss = -36.403301\nTotal_steps      45000: EpReward =        1.850896, Critic_Loss =   1.867366, Actor_Loss = -35.551537\nTotal_steps      46000: EpReward =      -34.900831, Critic_Loss =   1.436776, Actor_Loss = -36.391426\nTotal_steps      47000: EpReward =      428.068512, Critic_Loss =   2.092338, Actor_Loss = -37.320953\nTotal_steps      48000: EpReward =      322.527731, Critic_Loss =   1.956676, Actor_Loss = -37.960762\nTotal_steps      49000: EpReward =      190.533008, Critic_Loss =   1.914637, Actor_Loss = -38.915752\nTotal_steps      50000: EpReward =      -74.435212, Critic_Loss =   1.958324, Actor_Loss = -38.741043\nTotal_steps      51000: EpReward =       -7.991822, Critic_Loss =   1.873777, Actor_Loss = -38.414738\nTotal_steps      52000: EpReward =     1001.706421, Critic_Loss =   2.041497, Actor_Loss = -38.850143\nTotal_steps      53000: EpReward =     1122.236133, Critic_Loss =   2.953834, Actor_Loss = -41.217155\nTotal_steps      54000: EpReward =       59.833988, Critic_Loss =   2.191076, Actor_Loss = -39.705406\nTotal_steps      55000: EpReward =       80.152557, Critic_Loss =   1.979728, Actor_Loss = -40.367062\nTotal_steps      56000: EpReward =      -31.499809, Critic_Loss =   1.985309, Actor_Loss = -42.102806\nTotal_steps      57000: EpReward =     1527.002461, Critic_Loss =   2.307067, Actor_Loss = -42.703480\nTotal_steps      58000: EpReward =     1451.927122, Critic_Loss =   2.993318, Actor_Loss = -45.080402\nTotal_steps      59000: EpReward =     1538.025970, Critic_Loss =   2.831230, Actor_Loss = -46.150745\nTotal_steps      60000: EpReward =     1566.088669, Critic_Loss =   2.894365, Actor_Loss = -47.218781\nTotal_steps      61000: EpReward =     1738.417877, Critic_Loss =   3.075973, Actor_Loss = -47.729935\nTotal_steps      62000: EpReward =      -28.361293, Critic_Loss =   3.622001, Actor_Loss = -49.125408\nTotal_steps      63000: EpReward =     1830.704238, Critic_Loss =   3.948925, Actor_Loss = -50.155930\nTotal_steps      64000: EpReward =     1598.986668, Critic_Loss =   3.752213, Actor_Loss = -49.228905\nTotal_steps      65000: EpReward =     2168.619974, Critic_Loss =   3.466793, Actor_Loss = -51.854580\nTotal_steps      66000: EpReward =     2112.962140, Critic_Loss =   3.334447, Actor_Loss = -54.651382\nTotal_steps      67000: EpReward =     2383.243378, Critic_Loss =   4.926840, Actor_Loss = -54.946762\nTotal_steps      68000: EpReward =     2114.689881, Critic_Loss =   4.076775, Actor_Loss = -58.858105\nTotal_steps      69000: EpReward =     2440.390783, Critic_Loss =   5.362253, Actor_Loss = -59.135609\nTotal_steps      70000: EpReward =     2645.240833, Critic_Loss =   6.130947, Actor_Loss = -63.939121\nTotal_steps      71000: EpReward =     2467.830600, Critic_Loss =   5.100238, Actor_Loss = -63.338852\nTotal_steps      72000: EpReward =     2575.025300, Critic_Loss =   6.320094, Actor_Loss = -65.135597\nTotal_steps      73000: EpReward =     2652.855713, Critic_Loss =   6.582848, Actor_Loss = -71.204041\nTotal_steps      74000: EpReward =     2460.210350, Critic_Loss =   6.934016, Actor_Loss = -70.944214\nTotal_steps      75000: EpReward =     2753.915684, Critic_Loss =   4.697854, Actor_Loss = -70.360313\nTotal_steps      76000: EpReward =     2790.010547, Critic_Loss =   8.458378, Actor_Loss = -76.010506\nTotal_steps      77000: EpReward =     2725.149151, Critic_Loss =   7.556128, Actor_Loss = -76.840591\nTotal_steps      78000: EpReward =     2570.707919, Critic_Loss =   8.624098, Actor_Loss = -83.183861\nTotal_steps      79000: EpReward =     2899.445697, Critic_Loss =   8.012368, Actor_Loss = -87.400467\nTotal_steps      80000: EpReward =     2869.112933, Critic_Loss =   8.633027, Actor_Loss = -88.980377\nTotal_steps      81000: EpReward =     2806.465270, Critic_Loss =  10.238595, Actor_Loss = -90.000870\nTotal_steps      82000: EpReward =     2860.896795, Critic_Loss =   7.793625, Actor_Loss = -92.453346\nTotal_steps      83000: EpReward =     2866.875573, Critic_Loss =   7.479772, Actor_Loss = -97.691299\nTotal_steps      84000: EpReward =     2955.155394, Critic_Loss =   9.541733, Actor_Loss = -92.163528\nTotal_steps      85000: EpReward =     3004.335971, Critic_Loss =   9.694105, Actor_Loss = -96.779510\nTotal_steps      86000: EpReward =     3350.110059, Critic_Loss =  13.126915, Actor_Loss = -99.896317\nTotal_steps      87000: EpReward =     3353.666841, Critic_Loss =   7.617555, Actor_Loss = -102.005814\nTotal_steps      88000: EpReward =     3155.527140, Critic_Loss =  10.300079, Actor_Loss = -102.003853\nTotal_steps      89000: EpReward =     3446.011123, Critic_Loss =   8.037596, Actor_Loss = -104.130432\nTotal_steps      90000: EpReward =     3121.445428, Critic_Loss =   6.880720, Actor_Loss = -105.869934\nTotal_steps      91000: EpReward =     3248.504346, Critic_Loss =   8.362759, Actor_Loss = -111.520744\nTotal_steps      92000: EpReward =     3436.710192, Critic_Loss =  12.072423, Actor_Loss = -121.290405\nTotal_steps      93000: EpReward =     3585.537289, Critic_Loss =   9.478171, Actor_Loss = -110.182907\nTotal_steps      94000: EpReward =     3437.324059, Critic_Loss =  12.809259, Actor_Loss = -119.670609\nTotal_steps      95000: EpReward =     3618.730033, Critic_Loss =  11.687654, Actor_Loss = -123.263931\nTotal_steps      96000: EpReward =     3698.191041, Critic_Loss =   9.758541, Actor_Loss = -119.451080\nTotal_steps      97000: EpReward =     3541.423404, Critic_Loss =  12.953329, Actor_Loss = -132.118454\nTotal_steps      98000: EpReward =     3650.258550, Critic_Loss =   8.839127, Actor_Loss = -125.040573\nTotal_steps      99000: EpReward =     3625.478593, Critic_Loss =  10.618923, Actor_Loss = -130.851746\nTotal_steps     100000: EpReward =     3643.302293, Critic_Loss =  12.933092, Actor_Loss = -136.821167\nTotal_steps     101000: EpReward =     3873.145268, Critic_Loss =  11.031258, Actor_Loss = -137.888153\n"
    }
   ],
   "source": [
    "for total_numsteps, episode_reward, critic_loss, actor_loss in actor_critic_train(SAC, args):\n",
    "    print(\"Total_steps {:>10d}: EpReward = {:>15.6f}, Critic_Loss = {:>10.6f}, Actor_Loss = {:>10.6f}\".format(\n",
    "        total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}