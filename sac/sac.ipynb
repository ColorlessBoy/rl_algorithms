{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from model import ValueNetwork, QNetwork, GaussianPolicy, DeterministicPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replay Memory\n",
    "Replay memory stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "- Transition - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
    "- ReplayMemory - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'mask'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils: Soft_update and Hard_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAC\n",
    "\n",
    "The algorithm is a V-based method.\n",
    "\n",
    "- First we have three net: $V(s; \\theta_V)$, $Q(s, a; \\theta_Q)$ and $\\pi(a \\vert s; \\theta_\\pi)$.\n",
    "\n",
    "- We want $Q(s, a; \\theta_Q) = Q_V = \\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V) \\right)$;\n",
    "\n",
    "  $$\n",
    "  J(\\theta_Q) = \\mathbb{E}_{(s, a) \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} (\\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V)\\right) - Q(s, a; \\theta_Q) )^2 \\right\\};\n",
    "  $$\n",
    "\n",
    "- We want $\\pi(a \\vert s; \\theta_\\pi) = \\pi^*_{Q, soft}(a \\vert s) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a'; \\theta_Q)}$;\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  J(\\theta_\\pi) =& \\mathbb{E}_{s \\sim \\mathcal{D}}\\left\\{D_{KL} \\left(\\pi(\\cdot \\vert s; \\theta_\\pi) \\Vert \\pi^*_{Q, soft}(s, \\cdot) \\right)\\right\\} \\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - log(\\pi^*_{Q, soft}(a \\vert s))\\right\\}\\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - \\frac{1}{\\alpha} Q(s, a;\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  If we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$\n",
    "\n",
    "- We want $V(s; \\theta_V) = T^\\pi_{soft} V(s; \\theta_V) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]$;\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta_V) =& \\mathbb{E}_{s \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} \\left(V(s;\\theta_V) - \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]\\right)^2 \\right\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The paper use two Value-Net: $V(s; \\theta_V)$ and $\\hat V(s; \\theta_{\\hat V})$, and $\\theta_{\\hat V} = (1 - \\tau) \\theta_{\\hat V} + \\tau \\theta_V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.value = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        self.value_optim = Adam(self.value.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.value_target = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        hard_update(self.value_target, self.value)\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_target_batch = reward_batch + self.gamma * self.value_target(next_state_batch)\n",
    "\n",
    "            \n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, q_target_batch) + F.mse_loss(q2_batch, q_target_batch)\n",
    "\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            v_target = min_q_a - self.alpha * log_p_a\n",
    "        v = self.value(state_batch)\n",
    "        value_loss = F.mse_loss(v, v_target)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        self.value_optim.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.value_target, self.value, self.tau)\n",
    "        \n",
    "        return critic_loss + value_loss, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9f4ef0caa616>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9f4ef0caa616>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    - $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## SAC2\n",
    "\n",
    "- $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\n",
    "- $\\pi(a\\vert s; \\theta_\\pi) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a' ; \\theta_Q)}$.\n",
    "\n",
    "However we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC2(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action_batch, log_p_next_action_batch, _ = self.policy.sample(next_state_batch)\n",
    "            q1_next_target_batch, q2_next_target_batch = self.critic_target(next_state_batch, next_action_batch)\n",
    "            min_q_next_target_batch = torch.min(q1_next_target_batch, q2_next_target_batch) - self.alpha * log_p_next_action_batch\n",
    "            next_q_batch = reward_batch  + mask_batch * self.gamma * min_q_next_target_batch\n",
    "\n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, next_q_batch) + F.mse_loss(q2_batch, next_q_batch)\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        \n",
    "        return critic_loss, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Actor-Critic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_train(algorithm, args):\n",
    "    env = gym.make(args['env_name'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    env.seed(args['seed'])\n",
    "\n",
    "    agent = algorithm(env.observation_space.shape[0], env.action_space, args)\n",
    "    memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "    total_numsteps = 0\n",
    "    updates = 0\n",
    "\n",
    "    for i_epsisode in range(1, args['num_steps']+1):\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "        while not done:\n",
    "            if args['start_steps'] > total_numsteps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            \n",
    "            if len(memory) > args['batch_size']:\n",
    "                for i in range(args['updates_per_step']):\n",
    "                    critic_loss, actor_loss = agent.update_parameters(memory, args['batch_size'], updates)\n",
    "                    updates += 1\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_steps += 1\n",
    "            total_numsteps += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "            memory.push(state, action, reward, next_state, mask)\n",
    "            state = next_state\n",
    "        \n",
    "        yield total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'env_name'              : 'HalfCheetah-v2',\n",
    "    'policy_type'           : 'Gaussian',\n",
    "    'gamma'                 : 0.99,\n",
    "    'tau'                   : 0.005,\n",
    "    'lr'                    : 0.0003,\n",
    "    'alpha'                 : 0.2,\n",
    "    'seed'                  : 0,\n",
    "    'batch_size'            : 256,\n",
    "    'num_steps'             : 1000000,\n",
    "    'hidden_size'           : 256,\n",
    "    'updates_per_step'      : 1,\n",
    "    'start_steps'           : 10000,\n",
    "    'target_update_interval': 1,\n",
    "    'replay_size'           : 1000000,\n",
    "    'cuda'                  : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Total_steps       1000: EpReward =     -259.885259, Critic_Loss =   0.382607, Actor_Loss =  -1.951810\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-43d817ab4de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtotal_numsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactor_critic_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     print(\"Total_steps {:>10d}: EpReward = {:>15.6f}, Critic_Loss = {:>10.6f}, Actor_Loss = {:>10.6f}\".format(\n\u001b[1;32m      3\u001b[0m         \u001b[0mtotal_numsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     ))\n",
      "\u001b[0;32m<ipython-input-6-a8dc56dbf28f>\u001b[0m in \u001b[0;36mactor_critic_train\u001b[0;34m(algorithm, args)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'updates_per_step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d4f705fcdea1>\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdates\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for total_numsteps, episode_reward, critic_loss, actor_loss in actor_critic_train(SAC, args):\n",
    "    print(\"Total_steps {:>10d}: EpReward = {:>15.6f}, Critic_Loss = {:>10.6f}, Actor_Loss = {:>10.6f}\".format(\n",
    "        total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}