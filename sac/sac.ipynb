{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit1bdb8eb7d9a9428ab4af403dba516126",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from model import ValueNetwork, QNetwork, GaussianPolicy, DeterministicPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replay Memory\n",
    "Replay memory stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "- Transition - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
    "- ReplayMemory - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'mask'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils: Soft_update and Hard_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAC\n",
    "\n",
    "The algorithm is a V-based method.\n",
    "\n",
    "- First we have three net: $V(s; \\theta_V)$, $Q(s, a; \\theta_Q)$ and $\\pi(a \\vert s; \\theta_\\pi)$.\n",
    "\n",
    "- We want $Q(s, a; \\theta_Q) = Q_V = \\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V) \\right)$;\n",
    "\n",
    "  $$\n",
    "  J(\\theta_Q) = \\mathbb{E}_{(s, a) \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} (\\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V)\\right) - Q(s, a; \\theta_Q) )^2 \\right\\};\n",
    "  $$\n",
    "\n",
    "- We want $\\pi(a \\vert s; \\theta_\\pi) = \\pi^*_{Q, soft}(a \\vert s) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a'; \\theta_Q)}$;\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  J(\\theta_\\pi) =& \\mathbb{E}_{s \\sim \\mathcal{D}}\\left\\{D_{KL} \\left(\\pi(\\cdot \\vert s; \\theta_\\pi) \\Vert \\pi^*_{Q, soft}(s, \\cdot) \\right)\\right\\} \\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - log(\\pi^*_{Q, soft}(a \\vert s))\\right\\}\\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - \\frac{1}{\\alpha} Q(s, a;\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  If we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$\n",
    "\n",
    "- We want $V(s; \\theta_V) = T^\\pi_{soft} V(s; \\theta_V) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]$;\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta_V) =& \\mathbb{E}_{s \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} \\left(V(s;\\theta_V) - \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]\\right)^2 \\right\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The paper use two Value-Net: $V(s; \\theta_V)$ and $\\hat V(s; \\theta_{\\hat V})$, and $\\theta_{\\hat V} = (1 - \\tau) \\theta_{\\hat V} + \\tau \\theta_V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.value = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        self.value_optim = Adam(self.value.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.value_target = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        hard_update(self.value_target, self.value)\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_target_batch = reward_batch + self.gamma * self.value_target(next_state_batch)\n",
    "\n",
    "            \n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, q_target_batch) + F.mse_loss(q2_batch, q_target_batch)\n",
    "\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            v_target = min_q_a - self.alpha * log_p_a\n",
    "        v = self.value(state_batch)\n",
    "        value_loss = F.mse_loss(v, v_target)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        self.value_optim.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.value_target, self.value, self.tau)\n",
    "        \n",
    "        return critic_loss + value_loss, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9f4ef0caa616>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9f4ef0caa616>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    - $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## SAC2\n",
    "\n",
    "- $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\n",
    "- $\\pi(a\\vert s; \\theta_\\pi) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a' ; \\theta_Q)}$.\n",
    "\n",
    "However we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC2(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action_batch, log_p_next_action_batch, _ = self.policy.sample(next_state_batch)\n",
    "            q1_next_target_batch, q2_next_target_batch = self.critic_target(next_state_batch, next_action_batch)\n",
    "            min_q_next_target_batch = torch.min(q1_next_target_batch, q2_next_target_batch) - self.alpha * log_p_next_action_batch\n",
    "            next_q_batch = reward_batch  + mask_batch * self.gamma * min_q_next_target_batch\n",
    "\n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, next_q_batch) + F.mse_loss(q2_batch, next_q_batch)\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        \n",
    "        return critic_loss, policy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Actor-Critic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic_train(algorithm, args):\n",
    "    env = gym.make(args['env_name'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    env.seed(args['seed'])\n",
    "\n",
    "    agent = algorithm(env.observation_space.shape[0], env.action_space, args)\n",
    "    memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "    total_numsteps = 0\n",
    "    updates = 0\n",
    "\n",
    "    for i_epsisode in range(1, args['num_steps']+1):\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "        while not done:\n",
    "            if args['start_steps'] > total_numsteps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            \n",
    "            if len(memory) > args['batch_size']:\n",
    "                for i in range(args['updates_per_step']):\n",
    "                    critic_loss, actor_loss = agent.update_parameters(memory, args['batch_size'], updates)\n",
    "                    updates += 1\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_steps += 1\n",
    "            total_numsteps += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "            memory.push(state, action, reward, next_state, mask)\n",
    "            state = next_state\n",
    "        \n",
    "        yield total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'env_name'              : 'HalfCheetah-v2',\n",
    "    'policy_type'           : 'Gaussian',\n",
    "    'gamma'                 : 0.99,\n",
    "    'tau'                   : 0.005,\n",
    "    'lr'                    : 0.0003,\n",
    "    'alpha'                 : 0.2,\n",
    "    'seed'                  : 0,\n",
    "    'batch_size'            : 100,\n",
    "    'num_steps'             : 1000000,\n",
    "    'hidden_size'           : 256,\n",
    "    'updates_per_step'      : 1,\n",
    "    'start_steps'           : 10000,\n",
    "    'target_update_interval': 1,\n",
    "    'replay_size'           : 1000000,\n",
    "    'cuda'                  : True\n",
    "}"
   ]
  },
  {
   "source": [
    "for total_numsteps, episode_reward, critic_loss, actor_loss in actor_critic_train(SAC, args):\n",
    "    print(\"Total_steps {:>10d}: EpReward = {:>15.6f}, Critic_Loss = {:>10.6f}, Actor_Loss = {:>10.6f}\".format(\n",
    "        total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "    ))"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3.556136, Critic_Loss =   3.047202, Actor_Loss = -33.649033\nTotal_steps      33000: EpReward =       13.401999, Critic_Loss =   3.518537, Actor_Loss = -32.891869\nTotal_steps      34000: EpReward =      185.230455, Critic_Loss =   4.823693, Actor_Loss = -34.864418\nTotal_steps      35000: EpReward =      -72.859153, Critic_Loss =   3.153778, Actor_Loss = -35.856899\nTotal_steps      36000: EpReward =      767.623638, Critic_Loss =   3.291799, Actor_Loss = -34.900433\nTotal_steps      37000: EpReward =      858.033783, Critic_Loss =   2.781630, Actor_Loss = -35.462585\nTotal_steps      38000: EpReward =      -49.936063, Critic_Loss =   3.377226, Actor_Loss = -37.187977\nTotal_steps      39000: EpReward =      777.994755, Critic_Loss =   3.134436, Actor_Loss = -39.884617\nTotal_steps      40000: EpReward =       -9.701410, Critic_Loss =   3.997271, Actor_Loss = -40.041836\nTotal_steps      41000: EpReward =      164.914971, Critic_Loss =   3.440341, Actor_Loss = -38.187122\nTotal_steps      42000: EpReward =      767.808025, Critic_Loss =   3.752886, Actor_Loss = -38.819748\nTotal_steps      43000: EpReward =      206.010233, Critic_Loss =   4.123901, Actor_Loss = -39.278465\nTotal_steps      44000: EpReward =      399.725518, Critic_Loss =   3.962477, Actor_Loss = -41.529835\nTotal_steps      45000: EpReward =      281.210617, Critic_Loss =   3.881305, Actor_Loss = -42.074333\nTotal_steps      46000: EpReward =      243.592661, Critic_Loss =   3.543430, Actor_Loss = -42.492626\nTotal_steps      47000: EpReward =      938.447263, Critic_Loss =   4.185649, Actor_Loss = -40.850983\nTotal_steps      48000: EpReward =       60.477954, Critic_Loss =   5.391634, Actor_Loss = -46.299736\nTotal_steps      49000: EpReward =      389.207714, Critic_Loss =   7.686315, Actor_Loss = -45.641365\nTotal_steps      50000: EpReward =     1194.281386, Critic_Loss =   4.256754, Actor_Loss = -45.866970\nTotal_steps      51000: EpReward =     1453.345338, Critic_Loss =   5.262242, Actor_Loss = -46.031631\nTotal_steps      52000: EpReward =      299.627752, Critic_Loss =   6.484081, Actor_Loss = -46.670002\nTotal_steps      53000: EpReward =     1656.967173, Critic_Loss =   3.169218, Actor_Loss = -47.775608\nTotal_steps      54000: EpReward =     1701.657252, Critic_Loss =   4.776484, Actor_Loss = -50.280544\nTotal_steps      55000: EpReward =     1582.579721, Critic_Loss =   7.743119, Actor_Loss = -56.686367\nTotal_steps      56000: EpReward =     1561.657396, Critic_Loss =   5.728602, Actor_Loss = -53.827427\nTotal_steps      57000: EpReward =     2112.177290, Critic_Loss =   6.395512, Actor_Loss = -56.641647\nTotal_steps      58000: EpReward =     1865.559902, Critic_Loss =   4.837861, Actor_Loss = -60.614277\nTotal_steps      59000: EpReward =      345.946605, Critic_Loss =   5.510428, Actor_Loss = -53.482414\nTotal_steps      60000: EpReward =     1911.391587, Critic_Loss =  13.213818, Actor_Loss = -62.746548\nTotal_steps      61000: EpReward =     2042.606500, Critic_Loss =   8.025763, Actor_Loss = -65.542603\nTotal_steps      62000: EpReward =     2225.694491, Critic_Loss =   6.036143, Actor_Loss = -64.891411\nTotal_steps      63000: EpReward =     2360.251138, Critic_Loss =   8.879589, Actor_Loss = -67.073914\nTotal_steps      64000: EpReward =     2452.353032, Critic_Loss =  10.821815, Actor_Loss = -72.221275\nTotal_steps      65000: EpReward =     2451.526482, Critic_Loss =   6.345551, Actor_Loss = -69.195839\nTotal_steps      66000: EpReward =     2463.369486, Critic_Loss =   5.393919, Actor_Loss = -69.278999\nTotal_steps      67000: EpReward =     2708.354418, Critic_Loss =   9.666587, Actor_Loss = -77.149002\nTotal_steps      68000: EpReward =     2817.238303, Critic_Loss =  12.930425, Actor_Loss = -81.544975\nTotal_steps      69000: EpReward =     2722.886800, Critic_Loss =  11.680040, Actor_Loss = -83.834404\nTotal_steps      70000: EpReward =     2734.110010, Critic_Loss =   8.450639, Actor_Loss = -88.494316\nTotal_steps      71000: EpReward =     2639.262514, Critic_Loss =   9.544416, Actor_Loss = -87.002930\nTotal_steps      72000: EpReward =     2777.054861, Critic_Loss =  10.184373, Actor_Loss = -89.073067\nTotal_steps      73000: EpReward =     2908.060780, Critic_Loss =  17.231014, Actor_Loss = -92.219864\nTotal_steps      74000: EpReward =     3215.312066, Critic_Loss =  12.308206, Actor_Loss = -104.076080\nTotal_steps      75000: EpReward =     2861.632134, Critic_Loss =  11.220441, Actor_Loss = -96.641129\nTotal_steps      76000: EpReward =     3378.258142, Critic_Loss =  19.225151, Actor_Loss = -101.928940\nTotal_steps      77000: EpReward =     3124.249575, Critic_Loss =  12.941610, Actor_Loss = -103.788139\nTotal_steps      78000: EpReward =     3186.732590, Critic_Loss =  13.033768, Actor_Loss = -102.132263\nTotal_steps      79000: EpReward =     3263.814335, Critic_Loss =  14.321271, Actor_Loss = -108.953728\nTotal_steps      80000: EpReward =     3423.017496, Critic_Loss =  14.051668, Actor_Loss = -112.564842\nTotal_steps      81000: EpReward =     3426.553946, Critic_Loss =  17.283533, Actor_Loss = -116.006905\nTotal_steps      82000: EpReward =     3490.851272, Critic_Loss =  13.340101, Actor_Loss = -117.485878\nTotal_steps      83000: EpReward =     3472.462125, Critic_Loss =  19.317327, Actor_Loss = -120.800659\nTotal_steps      84000: EpReward =      297.395461, Critic_Loss =  15.864705, Actor_Loss = -112.518402\nTotal_steps      85000: EpReward =     3528.700533, Critic_Loss =  14.875881, Actor_Loss = -130.776749\nTotal_steps      86000: EpReward =     3506.148874, Critic_Loss =  21.745844, Actor_Loss = -132.531281\nTotal_steps      87000: EpReward =     2439.310923, Critic_Loss =  20.072206, Actor_Loss = -135.811829\nTotal_steps      88000: EpReward =     3523.786114, Critic_Loss =  15.731816, Actor_Loss = -133.015930\nTotal_steps      89000: EpReward =     3722.546388, Critic_Loss =  20.768372, Actor_Loss = -143.269623\nTotal_steps      90000: EpReward =     3805.170595, Critic_Loss =  17.702400, Actor_Loss = -141.911148\nTotal_steps      91000: EpReward =     3744.270362, Critic_Loss =  13.527726, Actor_Loss = -139.013870\nTotal_steps      92000: EpReward =     3856.185688, Critic_Loss =  20.278740, Actor_Loss = -136.069321\nTotal_steps      93000: EpReward =     3934.701712, Critic_Loss =  23.543917, Actor_Loss = -157.372437\nTotal_steps      94000: EpReward =     3761.605435, Critic_Loss =  17.827452, Actor_Loss = -149.584808\nTotal_steps      95000: EpReward =     3876.616287, Critic_Loss =  18.206690, Actor_Loss = -147.172272\nTotal_steps      96000: EpReward =     3845.823491, Critic_Loss =  20.823364, Actor_Loss = -161.846954\nTotal_steps      97000: EpReward =     3839.502439, Critic_Loss =  15.952453, Actor_Loss = -156.056046\nTotal_steps      98000: EpReward =     4086.687917, Critic_Loss =  19.088766, Actor_Loss = -166.239807\nTotal_steps      99000: EpReward =     4198.513074, Critic_Loss =  24.826874, Actor_Loss = -157.109039\nTotal_steps     100000: EpReward =     4037.294819, Critic_Loss =  16.002991, Actor_Loss = -162.695587\nTotal_steps     101000: EpReward =     4078.690117, Critic_Loss =  14.934737, Actor_Loss = -161.494644\nTotal_steps     102000: EpReward =     4163.226641, Critic_Loss =  17.727678, Actor_Loss = -167.484833\nTotal_steps     103000: EpReward =     3929.487009, Critic_Loss =  27.267691, Actor_Loss = -173.701614\nTotal_steps     104000: EpReward =     4080.410657, Critic_Loss =  21.306126, Actor_Loss = -163.142563\nTotal_steps     105000: EpReward =     4090.007730, Critic_Loss =  28.825666, Actor_Loss = -169.252335\nTotal_steps     106000: EpReward =     4071.691789, Critic_Loss =  23.745705, Actor_Loss = -182.472610\nTotal_steps     107000: EpReward =     4196.759003, Critic_Loss =  14.907740, Actor_Loss = -169.540863\nTotal_steps     108000: EpReward =     4519.640514, Critic_Loss =  17.982740, Actor_Loss = -171.398529\nTotal_steps     109000: EpReward =     4345.231980, Critic_Loss =  20.304934, Actor_Loss = -189.749817\nTotal_steps     110000: EpReward =     4546.589288, Critic_Loss =  22.234463, Actor_Loss = -192.282120\nTotal_steps     111000: EpReward =     4136.237374, Critic_Loss =  23.186541, Actor_Loss = -197.437073\nTotal_steps     112000: EpReward =     4529.914157, Critic_Loss =  23.823574, Actor_Loss = -196.605530\nTotal_steps     113000: EpReward =      924.153550, Critic_Loss =  34.367172, Actor_Loss = -197.145477\nTotal_steps     114000: EpReward =     4446.234722, Critic_Loss =  21.467731, Actor_Loss = -196.660889\nTotal_steps     115000: EpReward =     4449.313479, Critic_Loss =  18.469233, Actor_Loss = -209.763901\nTotal_steps     116000: EpReward =     4220.378572, Critic_Loss =  26.931637, Actor_Loss = -192.260773\nTotal_steps     117000: EpReward =     4678.624296, Critic_Loss =  22.307449, Actor_Loss = -195.605057\nTotal_steps     118000: EpReward =     4452.424145, Critic_Loss =  26.977694, Actor_Loss = -215.529327\nTotal_steps     119000: EpReward =     4368.993512, Critic_Loss =  28.111843, Actor_Loss = -203.835312\nTotal_steps     120000: EpReward =     4715.281707, Critic_Loss =  40.367302, Actor_Loss = -196.991425\nTotal_steps     121000: EpReward =     4339.837465, Critic_Loss =  22.303045, Actor_Loss = -217.917053\nTotal_steps     122000: EpReward =     4551.504014, Critic_Loss =  17.186531, Actor_Loss = -231.598236\nTotal_steps     123000: EpReward =     4755.560943, Critic_Loss =  20.405787, Actor_Loss = -217.147354\nTotal_steps     124000: EpReward =     4649.103460, Critic_Loss =  28.285179, Actor_Loss = -235.523468\nTotal_steps     125000: EpReward =     3290.426131, Critic_Loss =  23.529455, Actor_Loss = -224.982101\nTotal_steps     126000: EpReward =     4749.948157, Critic_Loss =  25.942955, Actor_Loss = -230.983917\nTotal_steps     127000: EpReward =     4866.317418, Critic_Loss =  20.090870, Actor_Loss = -206.941330\nTotal_steps     128000: EpReward =     4730.303381, Critic_Loss =  28.688696, Actor_Loss = -229.757599\nTotal_steps     129000: EpReward =     4787.529575, Critic_Loss =  29.544653, Actor_Loss = -242.835663\nTotal_steps     130000: EpReward =     4727.460010, Critic_Loss =  24.067211, Actor_Loss = -228.997070\nTotal_steps     131000: EpReward =     4966.491384, Critic_Loss =  13.205935, Actor_Loss = -241.694260\nTotal_steps     132000: EpReward =     4910.337883, Critic_Loss =  32.103748, Actor_Loss = -253.081619\nTotal_steps     133000: EpReward =     4587.362655, Critic_Loss =  72.292038, Actor_Loss = -236.581558\nTotal_steps     134000: EpReward =     4975.519914, Critic_Loss =  22.765553, Actor_Loss = -227.313507\nTotal_steps     135000: EpReward =     5163.686898, Critic_Loss =  24.800982, Actor_Loss = -247.519882\nTotal_steps     136000: EpReward =     5155.127762, Critic_Loss =  25.368610, Actor_Loss = -246.098038\nTotal_steps     137000: EpReward =     5165.111986, Critic_Loss =  28.578297, Actor_Loss = -265.270782\nTotal_steps     138000: EpReward =     5092.630982, Critic_Loss =  22.203184, Actor_Loss = -241.995041\nTotal_steps     139000: EpReward =     5144.065887, Critic_Loss =  27.832457, Actor_Loss = -255.983749\nTotal_steps     140000: EpReward =     5259.538548, Critic_Loss =  29.907459, Actor_Loss = -246.950684\nTotal_steps     141000: EpReward =     5424.527351, Critic_Loss =  24.885807, Actor_Loss = -245.946472\nTotal_steps     142000: EpReward =     5388.335393, Critic_Loss =  36.509109, Actor_Loss = -254.401443\nTotal_steps     143000: EpReward =     5301.989422, Critic_Loss =  19.430552, Actor_Loss = -249.704514\nTotal_steps     144000: EpReward =     3829.493130, Critic_Loss =  28.778206, Actor_Loss = -252.784363\nTotal_steps     145000: EpReward =     5418.222202, Critic_Loss =  19.949715, Actor_Loss = -262.062134\nTotal_steps     146000: EpReward =     5406.359975, Critic_Loss =  26.657204, Actor_Loss = -293.744476\nTotal_steps     147000: EpReward =     5574.795766, Critic_Loss =  27.165682, Actor_Loss = -281.734711\nTotal_steps     148000: EpReward =     5415.451484, Critic_Loss =  18.163916, Actor_Loss = -262.138184\nTotal_steps     149000: EpReward =     5556.607437, Critic_Loss =  20.228884, Actor_Loss = -270.737457\nTotal_steps     150000: EpReward =     5523.277003, Critic_Loss =  23.095503, Actor_Loss = -270.556549\nTotal_steps     151000: EpReward =     1399.975810, Critic_Loss =  25.355995, Actor_Loss = -251.926590\nTotal_steps     152000: EpReward =     5427.434843, Critic_Loss =  34.860485, Actor_Loss = -267.966431\nTotal_steps     153000: EpReward =     5847.209870, Critic_Loss =  41.882698, Actor_Loss = -283.280548\nTotal_steps     154000: EpReward =     5594.099078, Critic_Loss =  25.497250, Actor_Loss = -275.607483\nTotal_steps     155000: EpReward =     5784.691365, Critic_Loss =  34.152657, Actor_Loss = -291.359283\nTotal_steps     156000: EpReward =     5473.667452, Critic_Loss =  22.023514, Actor_Loss = -298.755615\nTotal_steps     157000: EpReward =     5892.139468, Critic_Loss =  23.091757, Actor_Loss = -276.518066\nTotal_steps     158000: EpReward =     5663.922718, Critic_Loss =  26.213390, Actor_Loss = -278.338867\nTotal_steps     159000: EpReward =     5675.603580, Critic_Loss =  34.595085, Actor_Loss = -277.752167\nTotal_steps     160000: EpReward =     5656.044854, Critic_Loss =  21.926468, Actor_Loss = -312.417328\nTotal_steps     161000: EpReward =     5630.215516, Critic_Loss =  28.954638, Actor_Loss = -301.252747\nTotal_steps     162000: EpReward =     5973.897396, Critic_Loss =  22.748852, Actor_Loss = -277.223053\nTotal_steps     163000: EpReward =     5700.674455, Critic_Loss =  19.152878, Actor_Loss = -288.682465\nTotal_steps     164000: EpReward =     6059.982526, Critic_Loss =  56.017632, Actor_Loss = -284.291718\nTotal_steps     165000: EpReward =     5892.242705, Critic_Loss =  41.446819, Actor_Loss = -307.564301\nTotal_steps     166000: EpReward =     5930.900665, Critic_Loss =  24.864716, Actor_Loss = -294.268524\nTotal_steps     167000: EpReward =     5904.428392, Critic_Loss =  27.409197, Actor_Loss = -314.518341\nTotal_steps     168000: EpReward =     5965.040842, Critic_Loss =  16.927111, Actor_Loss = -294.572388\nTotal_steps     169000: EpReward =     6079.994718, Critic_Loss =  18.976414, Actor_Loss = -295.645386\nTotal_steps     170000: EpReward =     5887.128460, Critic_Loss =  24.402370, Actor_Loss = -323.103119\nTotal_steps     171000: EpReward =     6078.018062, Critic_Loss =  26.545267, Actor_Loss = -298.996979\nTotal_steps     172000: EpReward =     5940.671390, Critic_Loss =  35.740025, Actor_Loss = -314.398651\nTotal_steps     173000: EpReward =     6020.600341, Critic_Loss =  29.164024, Actor_Loss = -309.472351\nTotal_steps     174000: EpReward =     6217.462894, Critic_Loss =  29.863003, Actor_Loss = -320.984985\nTotal_steps     175000: EpReward =     6278.921857, Critic_Loss =  30.160072, Actor_Loss = -309.922180\nTotal_steps     176000: EpReward =     6308.459299, Critic_Loss =  24.184219, Actor_Loss = -300.084412\nTotal_steps     177000: EpReward =     6087.520454, Critic_Loss =  19.051144, Actor_Loss = -320.471954\nTotal_steps     178000: EpReward =     6409.299377, Critic_Loss =  27.528111, Actor_Loss = -326.271820\nTotal_steps     179000: EpReward =     6335.108277, Critic_Loss =  27.605742, Actor_Loss = -339.655334\nTotal_steps     180000: EpReward =     6276.308073, Critic_Loss =  26.541431, Actor_Loss = -338.842987\nTotal_steps     181000: EpReward =     6201.716241, Critic_Loss =  27.964214, Actor_Loss = -320.403442\nTotal_steps     182000: EpReward =     6210.514190, Critic_Loss =  33.134792, Actor_Loss = -347.565857\nTotal_steps     183000: EpReward =     6063.791970, Critic_Loss =  32.813496, Actor_Loss = -333.733124\nTotal_steps     184000: EpReward =     6319.531923, Critic_Loss =  31.735798, Actor_Loss = -336.811554\nTotal_steps     185000: EpReward =     6046.775353, Critic_Loss =  32.103020, Actor_Loss = -328.132965\nTotal_steps     186000: EpReward =     6667.636662, Critic_Loss =  33.751987, Actor_Loss = -361.159576\nTotal_steps     187000: EpReward =     6398.755391, Critic_Loss =  45.148788, Actor_Loss = -340.691711\nTotal_steps     188000: EpReward =     6698.024165, Critic_Loss =  24.201345, Actor_Loss = -353.421936\nTotal_steps     189000: EpReward =     6437.338854, Critic_Loss =  22.082125, Actor_Loss = -332.508270\nTotal_steps     190000: EpReward =     6375.588998, Critic_Loss =  29.644352, Actor_Loss = -330.182831\nTotal_steps     191000: EpReward =     6486.664800, Critic_Loss =  15.418573, Actor_Loss = -341.803070\nTotal_steps     192000: EpReward =     6324.282944, Critic_Loss =  51.948917, Actor_Loss = -369.746582\nTotal_steps     193000: EpReward =     6336.665306, Critic_Loss =  36.918938, Actor_Loss = -365.200348\nTotal_steps     194000: EpReward =     6779.497672, Critic_Loss =  49.692284, Actor_Loss = -365.586945\nTotal_steps     195000: EpReward =     6411.075462, Critic_Loss =  26.426733, Actor_Loss = -366.176605\nTotal_steps     196000: EpReward =     6495.798607, Critic_Loss =  30.831573, Actor_Loss = -352.444031\nTotal_steps     197000: EpReward =     5579.086776, Critic_Loss =  42.082455, Actor_Loss = -367.788727\nTotal_steps     198000: EpReward =     5952.736670, Critic_Loss =  44.911156, Actor_Loss = -360.066559\nTotal_steps     199000: EpReward =     7126.013979, Critic_Loss =  48.315468, Actor_Loss = -358.514526\nTotal_steps     200000: EpReward =     7077.395780, Critic_Loss =  52.326962, Actor_Loss = -363.252960\nTotal_steps     201000: EpReward =     6772.851758, Critic_Loss =  63.943783, Actor_Loss = -351.903259\nTotal_steps     202000: EpReward =     6965.693568, Critic_Loss =  43.712234, Actor_Loss = -369.695312\nTotal_steps     203000: EpReward =     6657.497786, Critic_Loss =  80.409546, Actor_Loss = -391.147491\nTotal_steps     204000: EpReward =     2550.719267, Critic_Loss =  29.417904, Actor_Loss = -351.302185\nTotal_steps     205000: EpReward =     7143.302506, Critic_Loss =  51.357605, Actor_Loss = -379.389771\nTotal_steps     206000: EpReward =     6980.463440, Critic_Loss =  42.881855, Actor_Loss = -398.042114\nTotal_steps     207000: EpReward =     6873.029204, Critic_Loss =  24.295807, Actor_Loss = -374.923553\nTotal_steps     208000: EpReward =     6901.335295, Critic_Loss =  34.746460, Actor_Loss = -392.356018\nTotal_steps     209000: EpReward =     6931.557809, Critic_Loss =  76.930290, Actor_Loss = -376.290314\nTotal_steps     210000: EpReward =     7083.943287, Critic_Loss =  22.491911, Actor_Loss = -387.098419\nTotal_steps     211000: EpReward =     7048.022344, Critic_Loss = 131.635895, Actor_Loss = -378.453033\nTotal_steps     212000: EpReward =     7308.112293, Critic_Loss =  31.586592, Actor_Loss = -394.784302\nTotal_steps     213000: EpReward =     7026.613581, Critic_Loss = 151.061142, Actor_Loss = -377.058502\nTotal_steps     214000: EpReward =     7007.441052, Critic_Loss =  49.286800, Actor_Loss = -374.927521\nTotal_steps     215000: EpReward =     7209.983459, Critic_Loss =  41.307281, Actor_Loss = -398.458832\nTotal_steps     216000: EpReward =     7440.636342, Critic_Loss =  32.463791, Actor_Loss = -407.201721\nTotal_steps     217000: EpReward =     7474.032014, Critic_Loss =  69.018173, Actor_Loss = -387.821350\nTotal_steps     218000: EpReward =     7476.738974, Critic_Loss =  33.937153, Actor_Loss = -394.355621\nTotal_steps     219000: EpReward =     7371.977310, Critic_Loss =  35.219818, Actor_Loss = -416.991852\nTotal_steps     220000: EpReward =     7109.975589, Critic_Loss =  33.560226, Actor_Loss = -383.251862\nTotal_steps     221000: EpReward =     7189.829181, Critic_Loss =  37.266632, Actor_Loss = -395.352661\nTotal_steps     222000: EpReward =     7355.884877, Critic_Loss =  28.867561, Actor_Loss = -440.616791\nTotal_steps     223000: EpReward =     7091.143980, Critic_Loss =  35.612949, Actor_Loss = -404.585144\nTotal_steps     224000: EpReward =     7090.532719, Critic_Loss =  33.295593, Actor_Loss = -425.642365\nTotal_steps     225000: EpReward =     7292.725408, Critic_Loss =  61.081650, Actor_Loss = -416.639404\nTotal_steps     226000: EpReward =     7057.722508, Critic_Loss =  32.474400, Actor_Loss = -427.267639\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-43d817ab4de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtotal_numsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactor_critic_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     print(\"Total_steps {:>10d}: EpReward = {:>15.6f}, Critic_Loss = {:>10.6f}, Actor_Loss = {:>10.6f}\".format(\n\u001b[1;32m      3\u001b[0m         \u001b[0mtotal_numsteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     ))\n",
      "\u001b[0;32m<ipython-input-6-a8dc56dbf28f>\u001b[0m in \u001b[0;36mactor_critic_train\u001b[0;34m(algorithm, args)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'updates_per_step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d4f705fcdea1>\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}