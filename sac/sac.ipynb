{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bit1bdb8eb7d9a9428ab4af403dba516126",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "cells": [
  {
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from model import ValueNetwork, QNetwork, GaussianPolicy, DeterministicPolicy"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replay Memory\n",
    "Replay memory stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "For this, we're going to need two classses:\n",
    "\n",
    "- Transition - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.\n",
    "- ReplayMemory - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method for selecting a random batch of transitions for training."
   ]
  },
  {
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state', 'mask'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils: Soft_update and Hard_update"
   ]
  },
  {
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAC\n",
    "\n",
    "The algorithm is a V-based method.\n",
    "\n",
    "- First we have three net: $V(s; \\theta_V)$, $Q(s, a; \\theta_Q)$ and $\\pi(a \\vert s; \\theta_\\pi)$.\n",
    "\n",
    "- We want $Q(s, a; \\theta_Q) = Q_V = \\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V) \\right)$;\n",
    "\n",
    "  $$\n",
    "  J(\\theta_Q) = \\mathbb{E}_{(s, a) \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} (\\sum_{s'} p(s' \\vert s, a)\\left(r(s, a, s') + \\gamma V(s';\\theta_V)\\right) - Q(s, a; \\theta_Q) )^2 \\right\\};\n",
    "  $$\n",
    "\n",
    "- We want $\\pi(a \\vert s; \\theta_\\pi) = \\pi^*_{Q, soft}(a \\vert s) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a'; \\theta_Q)}$;\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  J(\\theta_\\pi) =& \\mathbb{E}_{s \\sim \\mathcal{D}}\\left\\{D_{KL} \\left(\\pi(\\cdot \\vert s; \\theta_\\pi) \\Vert \\pi^*_{Q, soft}(s, \\cdot) \\right)\\right\\} \\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - log(\\pi^*_{Q, soft}(a \\vert s))\\right\\}\\\\\n",
    "  =& \\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\pi(\\cdot \\vert s;\\theta_\\pi)} \\left\\{\\log(\\pi(a \\vert s;\\theta_\\pi)) - \\frac{1}{\\alpha} Q(s, a;\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  If we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$\n",
    "\n",
    "- We want $V(s; \\theta_V) = T^\\pi_{soft} V(s; \\theta_V) = \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]$;\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(\\theta_V) =& \\mathbb{E}_{s \\sim \\mathcal{D}} \\left\\{\\frac{1}{2} \\left(V(s;\\theta_V) - \\mathbb{E}_{a \\sim \\pi(\\cdot \\vert s; \\theta_\\pi)} \\left[Q(s, a; \\theta_Q)  - \\alpha \\log(\\pi(a \\vert s; \\theta_\\pi))  \\right]\\right)^2 \\right\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The paper use two Value-Net: $V(s; \\theta_V)$ and $\\hat V(s; \\theta_{\\hat V})$, and $\\theta_{\\hat V} = (1 - \\tau) \\theta_{\\hat V} + \\tau \\theta_V$"
   ]
  },
  {
   "source": [
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.value = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        self.value_optim = Adam(self.value.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.value_target = ValueNetwork(num_inputs, args['hidden_size']).to(self.device)\n",
    "        hard_update(self.value_target, self.value)\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_target_batch = reward_batch + self.gamma * self.value_target(next_state_batch)\n",
    "\n",
    "            \n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, q_target_batch) + F.mse_loss(q2_batch, q_target_batch)\n",
    "\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            v_target = min_q_a - self.alpha * log_p_a\n",
    "        v = self.value(state_batch)\n",
    "        value_loss = F.mse_loss(v, v_target)\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        self.value_optim.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.value_target, self.value, self.tau)\n",
    "        \n",
    "        return critic_loss + value_loss, policy_loss"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-9f4ef0caa616>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9f4ef0caa616>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    - $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## SAC2\n",
    "\n",
    "- $Q(s, a; \\theta_Q) = \\mathbb{E}_{s, a, s' \\sim \\mathcal{D}} [r(s, a, s') + \\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot \\vert s'; \\theta_\\pi)}(Q(s', a'; \\theta_{Q-target}) - \\alpha \\log\\pi(a' \\vert s'; \\theta_\\pi))]$;\n",
    "- $\\pi(a\\vert s; \\theta_\\pi) = \\frac{Q(s, a; \\theta_Q)}{\\sum_{a'} Q(s, a' ; \\theta_Q)}$.\n",
    "\n",
    "However we use Gaussian distribution in continuous action space:\n",
    "\n",
    "  $$\n",
    "  J(\\theta_\\pi) = \\mathbb{E}_{s \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}(0, 1)} \\left\\{\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} Q(s, f(s; \\epsilon, \\theta_\\pi);\\theta_Q) + \\log\\left(\\sum_a \\exp\\left\\{\\frac{1}{\\alpha} Q(s, a; \\theta_Q)\\right\\}\\right)\\right\\}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta_\\pi} J(\\theta_\\pi) = \\nabla_{\\theta_\\pi}\\log(\\pi(f(s; \\epsilon, \\theta_\\pi) \\vert s)) - \\frac{1}{\\alpha} \\nabla_{\\theta_\\pi} f(s; \\epsilon, \\theta_\\pi) \\nabla_a Q(s,a; \\theta_{Q})\\vert_{a = f(s; \\epsilon, \\theta_\\pi)}\n",
    "  $$"
   ]
  },
  {
   "source": [
    "class SAC2(object):\n",
    "    def __init__(self, num_inputs, action_space, args):\n",
    "        self.gamma = args['gamma']\n",
    "        self.tau = args['tau']\n",
    "        self.alpha = args['alpha']\n",
    "\n",
    "        self.policy_type = args['policy_type']\n",
    "        self.target_update_interval = args['target_update_interval']\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if args['cuda'] else \"cpu\")\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args['lr'])\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], args['hidden_size']).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            self.policy = GaussianPolicy(num_inputs, \n",
    "                                        action_space.shape[0],\n",
    "                                        args['hidden_size'],\n",
    "                                        action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "        else:\n",
    "            self.alpha = 0 # ???\n",
    "            self.policy = DeterministicPolicy(num_inputs, \n",
    "                                            action_space.shape[0],\n",
    "                                            args['hidden_size'],\n",
    "                                            action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=args['lr'])\n",
    "\n",
    "    def select_action(self, state, eval=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if eval == False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_action_batch, log_p_next_action_batch, _ = self.policy.sample(next_state_batch)\n",
    "            q1_next_target_batch, q2_next_target_batch = self.critic_target(next_state_batch, next_action_batch)\n",
    "            min_q_next_target_batch = torch.min(q1_next_target_batch, q2_next_target_batch) - self.alpha * log_p_next_action_batch\n",
    "            next_q_batch = reward_batch  + mask_batch * self.gamma * min_q_next_target_batch\n",
    "\n",
    "        q1_batch, q2_batch = self.critic(state_batch, action_batch)\n",
    "        critic_loss = F.mse_loss(q1_batch, next_q_batch) + F.mse_loss(q2_batch, next_q_batch)\n",
    "        a, log_p_a, _ = self.policy.sample(state_batch)\n",
    "        q1_a, q2_a = self.critic(state_batch, a)\n",
    "        min_q_a = torch.min(q1_a, q2_a)\n",
    "        policy_loss = ((self.alpha * log_p_a) - min_q_a).mean()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "        \n",
    "        return critic_loss, policy_loss"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Actor-Critic Training"
   ]
  },
  {
   "source": [
    "def actor_critic_train(algorithm, args):\n",
    "    env = gym.make(args['env_name'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    env.seed(args['seed'])\n",
    "\n",
    "    agent = algorithm(env.observation_space.shape[0], env.action_space, args)\n",
    "    memory = ReplayMemory(args['replay_size'])\n",
    "\n",
    "    total_numsteps = 0\n",
    "    updates = 0\n",
    "\n",
    "    for i_epsisode in range(1, args['num_steps']+1):\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        critic_loss = 0\n",
    "        actor_loss = 0\n",
    "        while not done:\n",
    "            if args['start_steps'] > total_numsteps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(state)\n",
    "            \n",
    "            if len(memory) > args['batch_size']:\n",
    "                for i in range(args['updates_per_step']):\n",
    "                    critic_loss, actor_loss = agent.update_parameters(memory, args['batch_size'], updates)\n",
    "                    updates += 1\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_steps += 1\n",
    "            total_numsteps += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "            memory.push(state, action, reward, next_state, mask)\n",
    "            state = next_state\n",
    "        \n",
    "        yield total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "\n",
    "    env.close()"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 6
  },
  {
   "source": [
    "args = {\n",
    "    'env_name'              : 'HalfCheetah-v2',\n",
    "    'policy_type'           : 'Gaussian',\n",
    "    'gamma'                 : 0.99,\n",
    "    'tau'                   : 0.005,\n",
    "    'lr'                    : 0.0003,\n",
    "    'alpha'                 : 0.2,\n",
    "    'seed'                  : 0,\n",
    "    'batch_size'            : 256,\n",
    "    'num_steps'             : 1000,\n",
    "    'hidden_size'           : 256,\n",
    "    'updates_per_step'      : 1,\n",
    "    'start_steps'           : 10000,\n",
    "    'target_update_interval': 1,\n",
    "    'replay_size'           : 1000000,\n",
    "    'cuda'                  : True\n",
    "}"
   ],
   "cell_type": "code",
   "outputs": [],
   "metadata": {},
   "execution_count": 7
  },
  {
   "source": [
    "for total_numsteps, episode_reward, critic_loss, actor_loss in actor_critic_train(SAC, args):\n",
    "    print(\"Total_steps {:>10d}: EpReward = {:>15.6f}, Critic_Loss = {:>10.6f}, Actor_Loss = {:>10.6f}\".format(\n",
    "        total_numsteps, episode_reward, critic_loss, actor_loss\n",
    "    ))"
   ],
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total_steps       1000: EpReward =     -302.805457, Critic_Loss =   0.186069, Actor_Loss =  -2.227973\nTotal_steps       2000: EpReward =     -351.978440, Critic_Loss =   0.616335, Actor_Loss =  -4.026294\nTotal_steps       3000: EpReward =     -398.831483, Critic_Loss =   0.986228, Actor_Loss =  -6.157232\nTotal_steps       4000: EpReward =     -230.631185, Critic_Loss =   3.133224, Actor_Loss =  -9.639139\nTotal_steps       5000: EpReward =      -91.389214, Critic_Loss =   2.061882, Actor_Loss = -13.141236\nTotal_steps       6000: EpReward =     -233.125782, Critic_Loss =   2.372377, Actor_Loss = -16.416943\nTotal_steps       7000: EpReward =     -340.925907, Critic_Loss =   2.959859, Actor_Loss = -20.124123\nTotal_steps       8000: EpReward =     -291.231577, Critic_Loss =   2.084315, Actor_Loss = -23.420652\nTotal_steps       9000: EpReward =     -331.284868, Critic_Loss =   3.521923, Actor_Loss = -25.879101\nTotal_steps      10000: EpReward =     -246.861674, Critic_Loss =   3.381457, Actor_Loss = -30.017651\nTotal_steps      11000: EpReward =     -304.263595, Critic_Loss =   4.485670, Actor_Loss = -32.607616\nTotal_steps      12000: EpReward =     -214.658866, Critic_Loss =   3.270874, Actor_Loss = -33.675873\nTotal_steps      13000: EpReward =     -228.334795, Critic_Loss =   4.239573, Actor_Loss = -34.309345\nTotal_steps      14000: EpReward =     -249.882099, Critic_Loss =   3.253756, Actor_Loss = -33.938240\nTotal_steps      15000: EpReward =     -257.564569, Critic_Loss =   2.930182, Actor_Loss = -33.858768\nTotal_steps      16000: EpReward =     -136.473643, Critic_Loss =   3.366944, Actor_Loss = -33.794708\nTotal_steps      17000: EpReward =     -183.743535, Critic_Loss =   2.642658, Actor_Loss = -33.097439\nTotal_steps      18000: EpReward =      -55.549017, Critic_Loss =   3.677439, Actor_Loss = -33.739773\nTotal_steps      19000: EpReward =       73.418707, Critic_Loss =   3.463598, Actor_Loss = -33.887650\nTotal_steps      20000: EpReward =      -90.717526, Critic_Loss =   3.093688, Actor_Loss = -33.612358\nTotal_steps      21000: EpReward =      224.532417, Critic_Loss =   3.033542, Actor_Loss = -34.391781\nTotal_steps      22000: EpReward =      156.206458, Critic_Loss =   3.090385, Actor_Loss = -34.083977\nTotal_steps      23000: EpReward =      296.271604, Critic_Loss =   3.372427, Actor_Loss = -33.305859\nTotal_steps      24000: EpReward =      190.903688, Critic_Loss =   2.528312, Actor_Loss = -33.373226\nTotal_steps      25000: EpReward =      668.975004, Critic_Loss =   3.221638, Actor_Loss = -36.218441\nTotal_steps      26000: EpReward =      637.036562, Critic_Loss =   3.489235, Actor_Loss = -37.753036\nTotal_steps      27000: EpReward =      -38.693173, Critic_Loss =   3.022428, Actor_Loss = -36.376472\nTotal_steps      28000: EpReward =      415.499982, Critic_Loss =   3.016587, Actor_Loss = -35.949284\nTotal_steps      29000: EpReward =      943.716903, Critic_Loss =   4.193105, Actor_Loss = -37.366325\nTotal_steps      30000: EpReward =      165.058879, Critic_Loss =   3.859028, Actor_Loss = -38.072548\nTotal_steps      31000: EpReward =     1044.301918, Critic_Loss =   2.878125, Actor_Loss = -39.324875\nTotal_steps      32000: EpReward =       60.623899, Critic_Loss =   3.955660, Actor_Loss = -42.094231\nTotal_steps      33000: EpReward =      818.856335, Critic_Loss =   3.715209, Actor_Loss = -40.591629\nTotal_steps      34000: EpReward =     1385.080536, Critic_Loss =   3.411758, Actor_Loss = -43.539070\nTotal_steps      35000: EpReward =     1277.441579, Critic_Loss =   4.032276, Actor_Loss = -42.737789\nTotal_steps      36000: EpReward =      580.771765, Critic_Loss =   3.780025, Actor_Loss = -43.130951\nTotal_steps      37000: EpReward =      268.779782, Critic_Loss =   4.927369, Actor_Loss = -44.662689\nTotal_steps      38000: EpReward =      577.069677, Critic_Loss =   4.175395, Actor_Loss = -44.314194\nTotal_steps      39000: EpReward =      218.372587, Critic_Loss =   5.118769, Actor_Loss = -47.532700\nTotal_steps      40000: EpReward =     1513.610242, Critic_Loss =   4.760620, Actor_Loss = -48.664299\nTotal_steps      41000: EpReward =     1387.556702, Critic_Loss =   5.585588, Actor_Loss = -47.527924\n"
    }
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}