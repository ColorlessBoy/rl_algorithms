{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from trpo import trpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "# Taken from\n",
    "# https://github.com/pytorch/tutorials/blob/master/Reinforcement%20(Q-)Learning%20with%20PyTorch.ipynb\n",
    "\n",
    "Transition = namedtuple('Transition', \n",
    "            ('state', 'action', 'reward', 'next_state', 'mask'))\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self):\n",
    "        return Transition(*zip(*self.memory))\n",
    "        # Very useful.\n",
    "        # The trpo is on-policy off-line algorithm.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args',\n",
    "        ('gamma', 'tau', 'damping', 'delta', 'cuda', 'hidden_size', 'lr'))\n",
    "trpo_args = Args(0.995, 1.0, 0.1, 0.02, True, (64,), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'HalfCheetah-v2'\n",
    "seed = 12345\n",
    "\n",
    "env = gym.make(env_name)\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "agent = trpo(env.observation_space.shape[0], env.action_space, trpo_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([ 6.7304e+27, -5.7420e+28, -1.4948e+28,  ..., -4.7447e+15,\n         1.1106e+14,  8.9008e+15], device='cuda:0')\ntensor([ 1.2500e+13, -9.5794e+13, -6.2780e+12,  ..., -2.0011e+13,\n        -1.0172e+10,  6.7982e+13], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\ntensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0')\nlinesearch failed\nepoch 1: reward -690.02\n"
    }
   ],
   "source": [
    "for epoch in range(1, 2):\n",
    "    memory = Memory()\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    done = False\n",
    "    while not done or len(memory) < 1000:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        mask = 0.0 if done else 1.0\n",
    "        memory.push(state, action, reward, next_state, mask)\n",
    "        reward_sum += reward\n",
    "        state = next_state\n",
    "    agent.update_parameters(memory)\n",
    "    print(\"epoch {}: reward {:.2f}\".format(epoch, reward_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}